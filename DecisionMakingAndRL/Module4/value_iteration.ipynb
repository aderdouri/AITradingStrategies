{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5f79e4600586cd7cc8dc23797e32385",
     "grade": false,
     "grade_id": "cell-2d16db69671e41fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment Description\n",
    "\n",
    "In this programming assignment, we will explore an agent in gridworld that uses value update to estimate the best action in a state.\n",
    "\n",
    "First, we import the required libraries.\n",
    "\n",
    "#### Note: Please do not modify any pre-defined variables. Doing so can affect the autograder results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f4518b4f9e72d5a27d977aa35f88569",
     "grade": false,
     "grade_id": "cell-0f4dd926c0e98cb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##importing modules \n",
    "!pip install --user gym==0.20.0\n",
    "import numpy as np\n",
    "import pprint\n",
    "from gridworld import GridworldEnv\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a gridworld environment from Sutton's Reinforcement Learning book chapter 4. \n",
    "\n",
    "You are an agent on an MxN grid, and your goal is to reach the terminal\n",
    "state at the top left or the bottom right corner.\n",
    "\n",
    "For example, a 4x4 grid looks as follows:\n",
    "\n",
    "        T  o  o  o\n",
    "        o  x  o  o\n",
    "        o  o  o  o\n",
    "        o  o  o  T\n",
    "\n",
    "x is your position, and Ts are the two terminal states.\n",
    "\n",
    "You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
    "\n",
    "You do not need to implement anything for it, but it will help you assess and take action through the grid. Note that\n",
    "\n",
    "- `env.P` represents the transition probabilities of the environment.\n",
    "- `env.P[s][a]` is a list of transition tuples (prob, next_state, reward, done).\n",
    "- `env.nS` is a number of states in the environment, 4 x 4 by default\n",
    "- `env.nA` is a number of actions in the environment, i.e. (UP=0, RIGHT=1, DOWN=2, LEFT=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41967b313281f04cf428d17368fb5cde",
     "grade": false,
     "grade_id": "cell-1e65aba4544cd796",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = GridworldEnv()  ###initializing the environments\n",
    "print(\"Environment Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "363af856a89fd728abf37c9ade1c43ec",
     "grade": false,
     "grade_id": "cell-faf80e8e0049a9ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we create a helper function `action_estimation` to calculate the action value for all actions in a given state.\n",
    "\n",
    "The arguments for the function are:\n",
    "- `env`: Grid world environment\n",
    "- `state`: The state to consider (int)\n",
    "- `V`: The value to use as an estimator, vector of length `env.nS`\n",
    "- `discount_factor`: Future discount factor\n",
    "\n",
    "The function should return:\n",
    "- A vector of length `env.nA` containing the expected value of each action.\n",
    "\n",
    "> HINT: How can the Bellman Update equation be used here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34c65324399b8d3f0bddc5b81f7d044d",
     "grade": false,
     "grade_id": "cell-2f158181b532578b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##action estimation function\n",
    "\n",
    "def action_estimation(env, state, V, discount_factor):\n",
    "    A = np.zeros(env.nA)\n",
    "    for a in range(env.nA):\n",
    "        for prob, next_state, reward, done in env.P[state][a]:\n",
    "            \n",
    "            # START CODING HERE\n",
    "        \n",
    "            # your code here\n",
    "            \n",
    "\n",
    "            # END CODING HERE\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "273150e9a15000f817eee719ffaa1fa0",
     "grade": false,
     "grade_id": "cell-24bc42dc701e9855",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we create a function `get_best_action` to find the best action and calculate the best action value.\n",
    "\n",
    "The arguments for the function are:\n",
    "- A vector of length `env.nA` containing the expected value of each action.\n",
    "\n",
    "The function should return a tuple with two values:\n",
    "- Best action `a`\n",
    "- Action value for `a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edcece65f8b640d2fb869694f0bab1ea",
     "grade": false,
     "grade_id": "cell-79ef4f20fc164650",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_best_action(A):\n",
    "    \n",
    "    #NOTE: If there are multiple states with equal maximum value, choose the first one in order UP, RIGHT, DOWN, LEFT\n",
    "    # For exampe if the values are [0, 2, 2, 1], your function should pick the direction as RIGHT as it appears before DOWN.\n",
    "    \n",
    "    # START CODING HERE\n",
    "        \n",
    "    # your code here\n",
    "    \n",
    "\n",
    "    # END CODING HERE    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9f27c8dcef98f45ef82d32e707211af",
     "grade": false,
     "grade_id": "cell-da614f15fd8a7509",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now create a function to create a deterministic policy using the optimal value function.\n",
    "\n",
    "The arguments for the function are:\n",
    "- `env`: Grid world environment\n",
    "- `V`: The value to use as an estimator, vector of length `env.nS`\n",
    "- `discount_factor`: Future discount factor\n",
    "\n",
    "The function should return a matrix `policy` of size `env.nS` x `env.nA`\n",
    "\n",
    "> Hint: A deterministic `policy` always selects one action per state for all `env.nS` states.\n",
    "\n",
    "> HINT: What should be the probability of best_action in the policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f67fc13917aa6ff4aa5312369d629f48",
     "grade": false,
     "grade_id": "cell-2dc9d98af1ca17cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_estimation(env, V, discount_factor):    \n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    for s in range(env.nS):\n",
    "        # find the best action for this state\n",
    "        A = action_estimation(env, s, V, discount_factor)\n",
    "        \n",
    "        best_action, best_action_value = get_best_action(A)\n",
    "\n",
    "        # START CODING HERE\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "\n",
    "        # END CODING HERE  \n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa8782fa018881af49b084d2d29bf88e",
     "grade": false,
     "grade_id": "cell-8114b9a4b522df35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, we implement the value iteration algorithm.\n",
    "\n",
    "The arguments for `value_iteration` are:\n",
    "- `env`: Gridworld environment\n",
    "- `theta`: We stop evaluation once our value function change is less than theta for all states\n",
    "- `discount_factor`: (Gamma) Future discount factor\n",
    "- `max_iterations`: A constant limit on how many times the update should be done\n",
    "\n",
    "And it should return:\n",
    "- A tuple (`policy`, `V`) of the optimal policy and the optimal value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9359ee2c2a7e34421b0f2a25d573ee22",
     "grade": false,
     "grade_id": "cell-0a5b2b5839dd8695",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0, max_iterations=1000):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        \n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = action_estimation(env, s, V, discount_factor)\n",
    "            \n",
    "            best_action, best_action_value = get_best_action(A)\n",
    "            \n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            \n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "            \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    policy = policy_estimation(env, V, discount_factor)\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f11682ae0cd98c6edfabd404fa35bead",
     "grade": false,
     "grade_id": "cell-66d7befdced22f11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's check the results of `value_iteration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, v = value_iteration(env)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0a13ae8fa46d93322d6e130c17420b8",
     "grade": false,
     "grade_id": "cell-c77622bddf92937c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you have implemented the aforementioned functions correctly, the following test should pass without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "644df84625e8a12fb8d963bdd6e8c615",
     "grade": true,
     "grade_id": "cell-bfdf444f701e05f6",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the value function\n",
    "expected_v = np.array([ 0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1,  0])\n",
    "assert np.allclose(v, expected_v, rtol=1e-4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "415f77d5ed404a708bbbb3500d534e83",
     "grade": false,
     "grade_id": "cell-7d661d0c3735f2a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You are all set.\n",
    "\n",
    "We run some hidden test cases on your code. You should be able to see the results once you submit the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5db7fa94c0ea3ba96afb98b5fc2fa778",
     "grade": true,
     "grade_id": "cell-624a8278382972c0",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82c64d27b31466f58f43ca8add581ef7",
     "grade": true,
     "grade_id": "cell-87d7c445045b467b",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "337a83f9f5505e3fe77270436a81f144",
     "grade": true,
     "grade_id": "cell-6f31fa320754ae14",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
