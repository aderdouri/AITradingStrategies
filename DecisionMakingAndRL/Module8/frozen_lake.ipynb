{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bea51f75897948428b550f5f29cdc851",
     "grade": false,
     "grade_id": "cell-baca4784e5651925",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Frozen Lake\n",
    "\n",
    "Frozen Lake environment is a 4Ã—4 grid which contains \n",
    "- S: initial state\n",
    "- F: frozen lake\n",
    "- H: hole\n",
    "- G: the goal\n",
    "The agent moves around the grid until it reaches the goal or the hole. If it falls into the hole, it has to start from the beginning and is rewarded the value 0. \n",
    "\n",
    "This environment is from OpenAI Gym, an open source Python library for developing and comparing reinforcement learning algorithms.\n",
    "\n",
    "#### Note: Please do not modify any pre-defined variables. Doing so can affect the autograder results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c9d85dfe514b50a11e028702c428dad",
     "grade": false,
     "grade_id": "cell-97b17caf2af4ef42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from time import sleep\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67994aaa21c25518a1bc85587fb0fce0",
     "grade": false,
     "grade_id": "cell-af6d30a0a70d1b75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install pygame\n",
    "import gym\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9430aeac39e6fb16fb3e3b04bc3247d1",
     "grade": false,
     "grade_id": "cell-8bf1d4c23b3c522b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "355928414a1221683ebcb6c4a3284970",
     "grade": false,
     "grade_id": "cell-17f9645b08024ffe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "Please complete the epsilon greedy function using the epsilon greedy formula from the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b70877bff343285c213b8aa4328a84a",
     "grade": false,
     "grade_id": "cell-5f3f01ea5b67a873",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate(OldEstimate, StepSize, Target):\n",
    "    'An incremental implementation of average.'\n",
    "    \n",
    "    NewEstimate = OldEstimate + StepSize * (Target - OldEstimate)\n",
    "    return NewEstimate\n",
    "\n",
    "def epsilon_greedy(value, e, seed = None):\n",
    "\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # START CODING HERE\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "    # END CODING HERE \n",
    "    \n",
    "    return action\n",
    "\n",
    "def action_evaluation(env, gamma, v):\n",
    "\n",
    "    nS = env.env.nS\n",
    "    nA = env.env.nA\n",
    "    q = np.zeros((nS, nA))\n",
    "    for s in range(nS):\n",
    "        for a in range(nA):\n",
    "            \n",
    "            for i in env.P[s][a]:\n",
    "                p, ns, r, ts = i\n",
    "                q[s,a] += p * (r + gamma * v[ns])\n",
    "\n",
    "    return q\n",
    "\n",
    "def action_selection(q):\n",
    "    \n",
    "    actions = np.argmax(q, axis = 1)    \n",
    "    return actions \n",
    "\n",
    "def render(env, policy):\n",
    "\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    \n",
    "    while not terminal:\n",
    "        action = policy[state]\n",
    "        state, reward, terminal, prob = env.step(action)\n",
    "        env.render()\n",
    "        sleep(1)\n",
    "    \n",
    "    print('Episode ends. Reward =', reward)\n",
    "    \n",
    "def human_play(env):\n",
    "    print('Action indices: LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3')\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    terminal = False\n",
    "    \n",
    "    while not terminal:\n",
    "        action = int(input('Give the environment your action index:'))\n",
    "        state, reward, terminal, prob = env.step(action)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fcba061f69872e765a7bb5a4d4f9bfd",
     "grade": false,
     "grade_id": "cell-8e89e692fdc69459",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can review the different concepts covered in the course below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19d5142b5766dbe143b2a08e5a1d43a6",
     "grade": false,
     "grade_id": "cell-572125a7354120b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma, max_iteration, theta):\n",
    "\n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros(env.nS, dtype = np.int32)\n",
    "    policy_stable = False\n",
    "    numIterations = 0\n",
    "    \n",
    "    while not policy_stable and numIterations < max_iteration:\n",
    "\n",
    "        V = policy_evaluation(env,policy,gamma,theta)\n",
    "        policy , policy_stable = policy_improvement(env, V, policy, gamma)\n",
    "\n",
    "        numIterations += 1\n",
    "        \n",
    "    return V, policy, numIterations\n",
    "\n",
    "\n",
    "def policy_evaluation(env, policy, gamma, theta):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    while True:\n",
    "        dl = 0 \n",
    "        ###from policy iteration pseudocode\n",
    "        for i in range(env.nS):\n",
    "            temp = V[i]\n",
    "            a = policy[i]\n",
    "            temp2 = 0\n",
    "            for p,ns,r,t in env.P[i][a]:\n",
    "                temp2 += (p * (r+gamma*V[ns]))\n",
    "            V[i]=temp2\n",
    "            dl = max(dl,abs(temp - V[i]))\n",
    "                \n",
    "               \n",
    "        if dl<theta:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(env, value_from_policy, policy, gamma):\n",
    "   \n",
    "    \n",
    "    policy_stable = True\n",
    "    for i in range(env.nS):\n",
    "        temp = policy[i]\n",
    "        \n",
    "        policy_final = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            \n",
    "            policy_temp = 0\n",
    "            for p,ns,r,t in env.P[i][a]:\n",
    "                policy_temp +=  p * (r + gamma * value_from_policy[ns])\n",
    "            policy_final[a] = policy_temp\n",
    "        policy[i] = np.argmax(policy_final)\n",
    "        \n",
    "        if temp !=policy[i]:\n",
    "            policy_stable = False\n",
    "\n",
    "    return policy, policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bc641d396ced661c4f95abe4e45c25b",
     "grade": false,
     "grade_id": "cell-3eda4e2b9a6701fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def QLearning(env, num_episodes, gamma, lr, e):\n",
    "\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    eps = 0\n",
    "    for i in range(num_episodes):\n",
    "        eps+=1\n",
    "        s = env.reset()\n",
    "        c = False\n",
    "        while not c:\n",
    "            a = epsilon_greedy(Q[s,:],e)\n",
    "            ns,r,done,_ = env.step(a)\n",
    "            Q[s,a] = Q[s,a] + lr * (r + gamma *  np.max(Q[ns,:]) - Q[s,a])\n",
    "            s = ns\n",
    "            c = done\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8ec4ba4b9fe208377bc696e305c5bd9",
     "grade": false,
     "grade_id": "cell-7cecedc3eabd0bea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def SARSA(env, num_episodes, gamma, lr, e):\n",
    "    \n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    eps = 0\n",
    "    for i in range(num_episodes):\n",
    "        eps+=1\n",
    "        s = env.reset()\n",
    "        a = epsilon_greedy(Q[s,:],e)\n",
    "        c = False\n",
    "        while not c:\n",
    "            ns,r,done,_ = env.step(a)\n",
    "            na = epsilon_greedy(Q[ns,:],e)\n",
    "            Q[s,a] = Q[s,a] + lr * (r + gamma * (Q[ns,na]) - Q[s,a])\n",
    "            s = ns\n",
    "            a = na\n",
    "            c = done\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2f2183eb2bb665f27ea21a38b46a8ac",
     "grade": false,
     "grade_id": "cell-2420240c7f672a91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, max_iteration, theta):\n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    numIterations = 0\n",
    "\n",
    "    while numIterations< max_iteration:\n",
    "        numIterations += 1\n",
    "        dl = 0 \n",
    "        for i in range(env.nS):\n",
    "            temp = V[i]\n",
    "            value_final = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                value_temp = 0\n",
    "                for p,ns,r,t in env.P[i][a]:\n",
    "                    value_temp +=  p * (r + gamma * V[ns])\n",
    "                value_final[a] = value_temp\n",
    "            V[i] = np.max(value_final)\n",
    "            dl = max(dl,abs(temp - V[i]))\n",
    "\n",
    "        if dl<theta:\n",
    "            break\n",
    "\n",
    "    policy = extract_policy(env, V, gamma)\n",
    "    \n",
    "    return V, policy, numIterations\n",
    "\n",
    "def extract_policy(env, v, gamma):\n",
    "\n",
    "    policy = np.zeros(env.nS, dtype = np.int32)\n",
    "\n",
    "    for i in range(env.nS):\n",
    "        policy_final = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            policy_temp = 0\n",
    "            for p,ns,r,t in env.P[i][a]:\n",
    "                policy_temp +=  p * (r + gamma * v[ns])\n",
    "            policy_final[a] = policy_temp\n",
    "        policy[i] = np.argmax(policy_final)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa94e889cb7ae12e7da332d2305090f3",
     "grade": false,
     "grade_id": "cell-620f56ae52cc5fb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "random.seed(6885)\n",
    "numTimeStep = 10000\n",
    "q_h = np.zeros(numTimeStep + 1) \n",
    "q_f = np.zeros(numTimeStep + 1) \n",
    "FixedStepSize = 0.5 \n",
    "for step in range(1, numTimeStep + 1):\n",
    "    if step < numTimeStep / 2:\n",
    "        r = random.gauss(mu = 1, sigma = 0.1)\n",
    "    else:\n",
    "        r = random.gauss(mu = 3, sigma = 0.1)\n",
    "\n",
    "    q_h[step] = estimate(q_h[step-1], 1/step , r)\n",
    "    q_f[step] = estimate(q_h[step-1], FixedStepSize , r)\n",
    "    \n",
    "q_h = q_h[1:]\n",
    "q_f = q_f[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "901f279107eedfb0717d5ba18071d650",
     "grade": false,
     "grade_id": "cell-caf28f4a8e0f9221",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(numTimeStep),q_h, label='Q_h')\n",
    "plt.plot(range(numTimeStep),q_f, label='Q_f')\n",
    "plt.ylabel('Q_values')\n",
    "plt.xlabel('Number of Steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5b2f26761dc334196e111e0aed8e4af",
     "grade": false,
     "grade_id": "cell-9904ae58b832eb7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(6885) #Set the seed to cancel the randomness\n",
    "q = np.random.normal(0, 1, size = 5)\n",
    "\n",
    "greedy_action = epsilon_greedy(q,0) #Use epsilon = 0 for Greedy\n",
    "e_greedy_action = epsilon_greedy(q,0.1) #Use epsilon = 0.1\n",
    "\n",
    "print('Values:')\n",
    "print(q)\n",
    "print('Greedy Choice =', greedy_action)\n",
    "print('Epsilon-Greedy Choice =', e_greedy_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "651f3e619cab80a3e76088b6c87c89af",
     "grade": true,
     "grade_id": "cell-20c6fbebd0821622",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
