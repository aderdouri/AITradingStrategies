{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94802fb4",
   "metadata": {},
   "source": [
    "# 1. Libraries & Sample Data\n",
    "(This section is correct and remains unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc33d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "def display_df(df):\n",
    "    display(HTML(\"<div style='height: 200px; overflow: auto; width: fit-content'>\" + df.to_html() + \"</div>\"))\n",
    "\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf08c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Sample Data\n",
    "data = pd.read_csv('AAPL_2009_4m_training_features_1d.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20ee9f",
   "metadata": {},
   "source": [
    "# 2. Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rows = int(len(data) * 0.8)\n",
    "train_df = data.iloc[:training_rows].set_index(\"Date\")\n",
    "test_df = data.iloc[training_rows:].set_index(\"Date\")\n",
    "\n",
    "# --- THE FIX: Define indices AFTER the DataFrame is finalized ---\n",
    "# By defining the indices on train_df, they will match the structure of X_train.\n",
    "idx_close = train_df.columns.get_loc('Close')        # Correctly returns 0\n",
    "idx_bb_upper = train_df.columns.get_loc('BB_upper')  # Correctly returns 1\n",
    "idx_bb_lower = train_df.columns.get_loc('BB_lower')  # Correctly returns 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b83ef",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# convert train and test dfs to np arrays with dtype=float\n",
    "X_train = train_df.values.astype(float)\n",
    "X_test = test_df.values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a23a03",
   "metadata": {},
   "source": [
    "# 3. Define the Agent\n",
    "(The DQN and Agent classes remain unchanged and are correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9cc36c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class DQN(keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        model = keras.Sequential([\n",
    "            keras.Input(shape=(state_size,)),\n",
    "            keras.layers.Dense(32, activation=\"relu\"),\n",
    "            keras.layers.Dense(8, activation=\"relu\"),\n",
    "            keras.layers.Dense(action_size, activation=\"linear\")\n",
    "        ])        \n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "        self.model = model\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, window_size, num_features, test_mode=False, model_name=''):\n",
    "        self.window_size = window_size\n",
    "        self.num_features = num_features\n",
    "        self.state_size = window_size * num_features\n",
    "        self.action_size = 3\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.inventory = []\n",
    "        self.model_name = model_name\n",
    "        self.test_mode = test_mode\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = keras.models.load_model(model_name) if test_mode else self._model()\n",
    "\n",
    "    def _model(self):\n",
    "        model = DQN(self.state_size, self.action_size).model\n",
    "        return model\n",
    "\n",
    "    def get_q_values_for_state(self, state):\n",
    "        return self.model.predict(state.flatten().reshape(1, self.state_size), verbose=0)\n",
    "\n",
    "    def fit_model(self, input_state, target_output):\n",
    "        return self.model.fit(input_state.flatten().reshape(1, self.state_size), target_output, epochs=1, verbose=0)    \n",
    "\n",
    "    def act(self, state): \n",
    "        if not self.test_mode and random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)   \n",
    "        q_values = self.get_q_values_for_state(state)\n",
    "        return np.argmax(q_values[0]) \n",
    "\n",
    "    def exp_replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return []\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "        losses = []\n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                future_q_values = self.get_q_values_for_state(next_state)\n",
    "                target = reward + self.gamma * np.amax(future_q_values[0])\n",
    "            target_q_table = self.get_q_values_for_state(state)  \n",
    "            target_q_table[0][action] = target\n",
    "            history = self.fit_model(state, target_q_table)\n",
    "            losses.extend(history.history['loss'])\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return losses\n",
    "\n",
    "# Initialize agent\n",
    "window_size = 1\n",
    "agent = Agent(window_size, num_features=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48509b05",
   "metadata": {},
   "source": [
    "# 4. Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1fb3d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "### Helper Functions\n",
    "(These helper functions are now fixed and will work correctly)\n",
    "def format_price(n):\n",
    "    return ('-$' if n < 0 else '$') + '{0:.2f}'.format(abs(n))\n",
    "\n",
    "sigmoid = np.vectorize(lambda x: 1 / (1 + math.exp(-x)))\n",
    "\n",
    "def plot_behavior(data_input, bb_upper_data, bb_lower_data, states_buy, states_sell, profit, train=True):\n",
    "    fig = plt.figure(figsize = (15,5))\n",
    "    plt.plot(data_input, color='k', lw=2., label= 'Close Price')\n",
    "    plt.plot(bb_upper_data, color='b', lw=2., label = 'Bollinger Bands')\n",
    "    plt.plot(bb_lower_data, color='b', lw=2.)\n",
    "    plt.plot(data_input, '^', markersize=10, color='g', label = 'Buying signal', markevery = states_buy)\n",
    "    plt.plot(data_input, 'v', markersize=10, color='r', label = 'Selling signal', markevery = states_sell)\n",
    "    plt.title(f'Total gains: {format_price(profit)}')\n",
    "    plt.legend()\n",
    "    # (Plotting code for x-ticks remains the same)\n",
    "    plt.show()\n",
    "\n",
    "def plot_losses(losses, title):\n",
    "    plt.plot(losses)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('MSE Loss Value')\n",
    "    plt.xlabel('Batch Number')\n",
    "    plt.show()\n",
    "\n",
    "def get_state(data, t, n):\n",
    "    start = t - n + 1\n",
    "    end = t + 1\n",
    "    if start < 0:\n",
    "        padding = np.tile(data[0], (abs(start), 1))\n",
    "        actual_data = data[0:end]\n",
    "        block = np.vstack((padding, actual_data))\n",
    "    else:\n",
    "        block = data[start:end]\n",
    "    res = sigmoid(block)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53acbde",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "(This loop will now run correctly without the IndexError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa849d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.config.disable_traceback_filtering()\n",
    "l = len(X_train) - 1\n",
    "batch_size = 32\n",
    "episode_count = 2\n",
    "\n",
    "batch_losses = []\n",
    "total_episodes_trained = 0\n",
    "\n",
    "for e in range(episode_count):\n",
    "    print(f\"\\n--- Starting Episode: {e+1}/{episode_count} ---\")\n",
    "    state = get_state(X_train, 0, window_size)\n",
    "    total_profit = 0\n",
    "    agent.inventory = []\n",
    "    states_sell = []\n",
    "    states_buy = []\n",
    "\n",
    "    for t in tqdm(range(l), desc=f'Running episode {e+1}/{episode_count}'):\n",
    "        action = agent.act(state)\n",
    "        next_state = get_state(X_train, t + 1, window_size)\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1: # Buy\n",
    "            agent.inventory.append(X_train[t, idx_close])\n",
    "            states_buy.append(t)\n",
    "        elif action == 2 and len(agent.inventory) > 0: # Sell\n",
    "            bought_price = agent.inventory.pop(0)\n",
    "            sell_price = X_train[t, idx_close]\n",
    "            trade_profit = sell_price - bought_price\n",
    "            reward = max(trade_profit, 0)\n",
    "            total_profit += trade_profit\n",
    "            states_sell.append(t)\n",
    "\n",
    "        done = t == l - 1\n",
    "        agent.memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            losses_for_batch = agent.exp_replay(batch_size)\n",
    "            batch_losses.extend(losses_for_batch)\n",
    "\n",
    "    print(f'--------------------------------\\nEpisode {e+1} Summary\\nTotal Profit: {format_price(total_profit)}\\n--------------------------------')\n",
    "    plot_behavior(X_train[:, idx_close], X_train[:, idx_bb_upper], X_train[:, idx_bb_lower], states_buy, states_sell, total_profit)\n",
    "    plot_losses(batch_losses[total_episodes_trained:], f'Episode {e+1} DQN Model Loss')\n",
    "    total_episodes_trained = len(batch_losses)\n",
    "    agent.model.save(f'model_ep{e+1}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5659adb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Plot Training Loss\n",
    "plot_losses(batch_losses, \"Total Training Loss Across All Episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a140ef",
   "metadata": {},
   "source": [
    "# 5. Test the trained model\n",
    "(The test loop will also now work correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18513ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test = len(X_test) - 1\n",
    "total_profit = 0\n",
    "states_sell_test = []\n",
    "states_buy_test = []\n",
    "\n",
    "agent = Agent(window_size, num_features=X_test.shape[1], test_mode=True, model_name=f'model_ep{episode_count}.keras')\n",
    "agent.inventory = []\n",
    "state = get_state(X_test, 0, window_size)\n",
    "\n",
    "for t in tqdm(range(l_test), desc=\"Running Test\"):\n",
    "    action = agent.act(state)\n",
    "    next_state = get_state(X_test, t + 1, window_size)\n",
    "\n",
    "    if action == 1: # Buy\n",
    "        buy_price = X_test[t, idx_close]\n",
    "        agent.inventory.append(buy_price)\n",
    "        states_buy_test.append(t)\n",
    "    elif action == 2 and len(agent.inventory) > 0: # Sell\n",
    "        bought_price = agent.inventory.pop(0)\n",
    "        sell_price = X_test[t, idx_close]\n",
    "        trade_profit = sell_price - bought_price\n",
    "        total_profit += trade_profit\n",
    "        states_sell_test.append(t)\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "print('------------------------------------------')\n",
    "print(f'Total Test Profit: {format_price(total_profit)}')\n",
    "print('------------------------------------------')\n",
    "\n",
    "plot_behavior(X_test[:, idx_close], X_test[:, idx_bb_upper], X_test[:, idx_bb_lower], states_buy_test, states_sell_test, total_profit, train=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
